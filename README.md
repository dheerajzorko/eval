# eval
# Evaluate your LLM Model outputs

# Overview
This project leverages deepeval, an LLM output evaluation framework to 
function.

## Features

## Installation

1. Install Deepeval
### [deepeval documentation](https://docs.confident-ai.com/docs/getting-started)
```shell//
pip install -u deepeval
deepeval login --confident-api-key <YOUR_DEEPEVAL_KEY>
```

## Dependencies

Make sure you have OPENAI API KEY set 
```shell//
export OPENAI_API_KEY=<YOUR_OPEN_AI_KEY>
```

## Usage
how to run deepeval evaluation
```shell//
deepeval test run <PROGRAM.py>
```

## How it works
TBD

## Notes
<To be Added>
